{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Assignment - Intentionally Blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries should be installed before proceeding further:\n",
    "- pandas\n",
    "- numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the available datasets from the team assignment the original dataset is read for additional information about each ride. The additional information contains the geographical coordinates for the start position and the end position of each ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in weather and philadelphia_2017 data\n",
    "df_weather = pd.read_csv(\"data/weather_hourly_philadelphia.csv\")\n",
    "df_philadelphia_2017 = pd.read_csv(\"data/philadelphia_2017.csv\")\n",
    "\n",
    "# Additionally read the philadelphia data from the official website\n",
    "data_philadelphia_2017_Q1 = pd.read_csv(\"data/indego_Q1_2017.csv\")\n",
    "data_philadelphia_2017_Q2 = pd.read_csv(\"data/indego_Q2_2017.csv\")\n",
    "data_philadelphia_2017_Q3 = pd.read_csv(\"data/indego_Q3_2017.csv\")\n",
    "data_philadelphia_2017_Q4 = pd.read_csv(\"data/indego_Q4_2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing and cleaning the philadelphia datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all quarters of philadelphia data and convert them to a dataframe\n",
    "df_philadelphia_website = pd.concat([data_philadelphia_2017_Q1, data_philadelphia_2017_Q2, data_philadelphia_2017_Q3, data_philadelphia_2017_Q4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns duration, start_station_id, end_station_id, plan_duration, trip_route_category, passholder_type, start_station, end_station\n",
    "df_philadelphia_website.drop(['duration', 'start_station_id', 'end_station_id', 'plan_duration', 'trip_route_category', 'passholder_type', 'start_station', 'end_station', 'trip_id'], axis=1, inplace=True)\n",
    "df_philadelphia_website\n",
    "\n",
    "#Changing the type of start_time, end_time (df_philadelphia_2017 and df_philadelphia_website) and date_time (df_weather) to datetime type from pandas\n",
    "df_philadelphia_website.loc[:,'start_time'] = pd.to_datetime(df_philadelphia_website['start_time'])\n",
    "df_philadelphia_website.loc[:,'end_time'] = pd.to_datetime(df_philadelphia_website['end_time'])\n",
    "\n",
    "df_philadelphia_2017.loc[:,'start_time'] = pd.to_datetime(df_philadelphia_2017['start_time'])\n",
    "df_philadelphia_2017.loc[:,\"end_time\"] = pd.to_datetime(df_philadelphia_2017[\"end_time\"])\n",
    "\n",
    "#Sort dataframes by their corresponding time column\n",
    "df_philadelphia_website.sort_values([\"start_time\"], inplace = True)\n",
    "df_philadelphia_2017.sort_values([\"start_time\"], inplace = True)\n",
    "\n",
    "\n",
    "#Resetting the indexes\n",
    "df_philadelphia_website.reset_index(drop = True, inplace = True)\n",
    "df_philadelphia_2017.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we further clean the dataset for philadelphia its important to mention that we first deleted the irrelevant stations because some duplicate rows happen to have the same start time, end time and bike ids but different start and end stations like the \"Virtual Station\". Therefore we deleted those stations first to avoid deleting useful records. In the following example you can see that deleting the duplicates first could lead to deleting useful records and keeping irrelevant records which are deleted either way later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_station_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255753</th>\n",
       "      <td>2017-06-05 15:26:00</td>\n",
       "      <td>2017-06-05 15:27:00</td>\n",
       "      <td>3152</td>\n",
       "      <td>3152</td>\n",
       "      <td>11907</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>40th &amp; Baltimore, Trolley Portal</td>\n",
       "      <td>40th &amp; Baltimore, Trolley Portal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255755</th>\n",
       "      <td>2017-06-05 15:26:00</td>\n",
       "      <td>2017-06-05 15:27:00</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>11907</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>Virtual Station</td>\n",
       "      <td>Virtual Station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256143</th>\n",
       "      <td>2017-06-05 17:26:00</td>\n",
       "      <td>2017-06-05 17:31:00</td>\n",
       "      <td>3152</td>\n",
       "      <td>3024</td>\n",
       "      <td>11907</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>40th &amp; Baltimore, Trolley Portal</td>\n",
       "      <td>43rd &amp; Chester, Clark Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256148</th>\n",
       "      <td>2017-06-05 17:26:00</td>\n",
       "      <td>2017-06-05 17:31:00</td>\n",
       "      <td>3000</td>\n",
       "      <td>3024</td>\n",
       "      <td>11907</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Virtual Station</td>\n",
       "      <td>43rd &amp; Chester, Clark Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258113</th>\n",
       "      <td>2017-06-06 16:02:00</td>\n",
       "      <td>2017-06-06 16:33:00</td>\n",
       "      <td>3004</td>\n",
       "      <td>3000</td>\n",
       "      <td>5314</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>Municipal Services Building Plaza</td>\n",
       "      <td>Virtual Station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258115</th>\n",
       "      <td>2017-06-06 16:02:00</td>\n",
       "      <td>2017-06-06 16:33:00</td>\n",
       "      <td>3004</td>\n",
       "      <td>3161</td>\n",
       "      <td>5314</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>Municipal Services Building Plaza</td>\n",
       "      <td>30th Street Station East</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time            end_time  start_station_id  \\\n",
       "255753 2017-06-05 15:26:00 2017-06-05 15:27:00              3152   \n",
       "255755 2017-06-05 15:26:00 2017-06-05 15:27:00              3000   \n",
       "256143 2017-06-05 17:26:00 2017-06-05 17:31:00              3152   \n",
       "256148 2017-06-05 17:26:00 2017-06-05 17:31:00              3000   \n",
       "258113 2017-06-06 16:02:00 2017-06-06 16:33:00              3004   \n",
       "258115 2017-06-06 16:02:00 2017-06-06 16:33:00              3004   \n",
       "\n",
       "        end_station_id  bike_id user_type                 start_station_name  \\\n",
       "255753            3152    11907  Indego30   40th & Baltimore, Trolley Portal   \n",
       "255755            3000    11907  Indego30                    Virtual Station   \n",
       "256143            3024    11907   Walk-up   40th & Baltimore, Trolley Portal   \n",
       "256148            3024    11907   Walk-up                    Virtual Station   \n",
       "258113            3000     5314  Indego30  Municipal Services Building Plaza   \n",
       "258115            3161     5314  Indego30  Municipal Services Building Plaza   \n",
       "\n",
       "                        end_station_name  \n",
       "255753  40th & Baltimore, Trolley Portal  \n",
       "255755                   Virtual Station  \n",
       "256143        43rd & Chester, Clark Park  \n",
       "256148        43rd & Chester, Clark Park  \n",
       "258113                   Virtual Station  \n",
       "258115          30th Street Station East  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_philadelphia_2017[df_philadelphia_2017.duplicated(subset=['start_time', 'end_time', 'bike_id'], keep=False)].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"3083\" station does not exist as a real station. Furthermore the \"Virtual Station\" is used by staff to check in or check out a bike remotely for an event or when the bike was not properly checked in or out. Therefore we check the number of rows where one of those stations occur and delete those rows afterwards.\n",
    "\n",
    "For further information please check the official site about the data and the station table [here](https://www.rideindego.com/about/data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_philadelphia_2017 with \"Virtual Station\" and \"3083\" in the start station or end station:  6512\n"
     ]
    }
   ],
   "source": [
    "print(r'Number of rows in df_philadelphia_2017 with \"Virtual Station\" and \"3083\" in the start station or end station: ',\n",
    " df_philadelphia_2017[(df_philadelphia_2017[\"start_station_name\"] == \"Virtual Station\") | (df_philadelphia_2017[\"end_station_name\"] == \"Virtual Station\") |\n",
    "  (df_philadelphia_2017[\"start_station_name\"] == \"3083\") | (df_philadelphia_2017[\"end_station_name\"] == \"3083\")\n",
    "  ].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_station_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_time, end_time, start_station_id, end_station_id, bike_id, user_type, start_station_name, end_station_name]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deleting \"Virtual Station\" and \"3083\" from the start_station_name and end_station_name columns\n",
    "df_philadelphia_2017 = df_philadelphia_2017[(df_philadelphia_2017[\"start_station_name\"] != \"3083\") & (df_philadelphia_2017[\"end_station_name\"] != \"3083\") ]\n",
    "df_philadelphia_2017 = df_philadelphia_2017[(df_philadelphia_2017[\"start_station_name\"] != \"Virtual Station\") & (df_philadelphia_2017[\"end_station_name\"] != \"Virtual Station\") ]\n",
    "\n",
    "display(df_philadelphia_2017[(df_philadelphia_2017[\"start_station_name\"] == \"Virtual Station\") | (df_philadelphia_2017[\"end_station_name\"] == \"Virtual Station\") | (df_philadelphia_2017[\"start_station_name\"] == \"3083\") | (df_philadelphia_2017[\"end_station_name\"] == \"3083\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"bike_id\" should be unique to one bike. No trip with the same bike_id should start and end on the same time interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in df_philadelphia_2017:  55\n",
      "Number of duplicates in df_philadelphia_website:  68\n"
     ]
    }
   ],
   "source": [
    "#Checking for any duplicates in philadelphia data\n",
    "print(\"Number of duplicates in df_philadelphia_2017: \", df_philadelphia_2017.duplicated(subset=['start_time', 'end_time', 'bike_id']).sum())\n",
    "print(\"Number of duplicates in df_philadelphia_website: \", df_philadelphia_website.duplicated(subset=['start_time', 'end_time', 'bike_id']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_station_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_time, end_time, start_station_id, end_station_id, bike_id, user_type, start_station_name, end_station_name]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>bike_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_time, end_time, start_lat, start_lon, end_lat, end_lon, bike_id]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dropping all duplicates for the philadelphia data depending on the start_time, end_time and bike_id\n",
    "df_philadelphia_2017.drop_duplicates(subset=[\"start_time\", \"end_time\", \"bike_id\"], keep=\"first\", inplace=True)\n",
    "df_philadelphia_website.drop_duplicates(subset=[\"start_time\", \"end_time\", \"bike_id\"], keep=\"first\", inplace=True)\n",
    "\n",
    "display(df_philadelphia_2017[df_philadelphia_2017.duplicated(subset=['start_time', 'end_time', 'bike_id'])].head(10))\n",
    "display(df_philadelphia_website[df_philadelphia_website.duplicated(subset=['start_time', 'end_time', 'bike_id'])].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:05:00</td>\n",
       "      <td>2017-01-01 00:16:00</td>\n",
       "      <td>3046</td>\n",
       "      <td>3041</td>\n",
       "      <td>5347</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>2nd &amp; Market</td>\n",
       "      <td>Girard Station, MFL</td>\n",
       "      <td>39.950119</td>\n",
       "      <td>-75.144722</td>\n",
       "      <td>39.968491</td>\n",
       "      <td>-75.135460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:21:00</td>\n",
       "      <td>2017-01-01 00:57:00</td>\n",
       "      <td>3110</td>\n",
       "      <td>3054</td>\n",
       "      <td>3364</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Del. River Trail &amp; Penn St.</td>\n",
       "      <td>Rodin Museum</td>\n",
       "      <td>39.961750</td>\n",
       "      <td>-75.136414</td>\n",
       "      <td>39.962502</td>\n",
       "      <td>-75.174202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:22:00</td>\n",
       "      <td>2017-01-01 00:57:00</td>\n",
       "      <td>3110</td>\n",
       "      <td>3054</td>\n",
       "      <td>2536</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Del. River Trail &amp; Penn St.</td>\n",
       "      <td>Rodin Museum</td>\n",
       "      <td>39.961750</td>\n",
       "      <td>-75.136414</td>\n",
       "      <td>39.962502</td>\n",
       "      <td>-75.174202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:27:00</td>\n",
       "      <td>2017-01-01 00:39:00</td>\n",
       "      <td>3041</td>\n",
       "      <td>3005</td>\n",
       "      <td>5176</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>Girard Station, MFL</td>\n",
       "      <td>Welcome Park, NPS</td>\n",
       "      <td>39.968491</td>\n",
       "      <td>-75.135460</td>\n",
       "      <td>39.947330</td>\n",
       "      <td>-75.144028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:28:00</td>\n",
       "      <td>2017-01-01 00:36:00</td>\n",
       "      <td>3047</td>\n",
       "      <td>3124</td>\n",
       "      <td>5370</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Independence Mall, NPS</td>\n",
       "      <td>Race Street Pier</td>\n",
       "      <td>39.950710</td>\n",
       "      <td>-75.149208</td>\n",
       "      <td>39.952950</td>\n",
       "      <td>-75.139793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782335</th>\n",
       "      <td>2017-12-31 23:05:00</td>\n",
       "      <td>2017-12-31 23:33:00</td>\n",
       "      <td>3070</td>\n",
       "      <td>3124</td>\n",
       "      <td>3708</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>Spring Garden Station, MFL</td>\n",
       "      <td>Race Street Pier</td>\n",
       "      <td>39.960621</td>\n",
       "      <td>-75.139832</td>\n",
       "      <td>39.952950</td>\n",
       "      <td>-75.139793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782336</th>\n",
       "      <td>2017-12-31 23:11:00</td>\n",
       "      <td>2018-01-01 11:03:00</td>\n",
       "      <td>3107</td>\n",
       "      <td>3165</td>\n",
       "      <td>5117</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>33rd &amp; Reservoir</td>\n",
       "      <td>24th &amp; Race SRT</td>\n",
       "      <td>39.982029</td>\n",
       "      <td>-75.188660</td>\n",
       "      <td>39.958191</td>\n",
       "      <td>-75.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782337</th>\n",
       "      <td>2017-12-31 23:18:00</td>\n",
       "      <td>2017-12-31 23:25:00</td>\n",
       "      <td>3033</td>\n",
       "      <td>3046</td>\n",
       "      <td>11933</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>10th &amp; Chestnut</td>\n",
       "      <td>2nd &amp; Market</td>\n",
       "      <td>39.950050</td>\n",
       "      <td>-75.156723</td>\n",
       "      <td>39.950119</td>\n",
       "      <td>-75.144722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782338</th>\n",
       "      <td>2017-12-31 23:39:00</td>\n",
       "      <td>2017-12-31 23:40:00</td>\n",
       "      <td>3163</td>\n",
       "      <td>3163</td>\n",
       "      <td>6725</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>25th &amp; Locust</td>\n",
       "      <td>25th &amp; Locust</td>\n",
       "      <td>39.949741</td>\n",
       "      <td>-75.180969</td>\n",
       "      <td>39.949741</td>\n",
       "      <td>-75.180969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782339</th>\n",
       "      <td>2017-12-31 23:41:00</td>\n",
       "      <td>2017-12-31 23:55:00</td>\n",
       "      <td>3163</td>\n",
       "      <td>3057</td>\n",
       "      <td>11864</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>25th &amp; Locust</td>\n",
       "      <td>Philadelphia Museum of Art</td>\n",
       "      <td>39.949741</td>\n",
       "      <td>-75.180969</td>\n",
       "      <td>39.964581</td>\n",
       "      <td>-75.180031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782340 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time            end_time  start_station_id  \\\n",
       "0      2017-01-01 00:05:00 2017-01-01 00:16:00              3046   \n",
       "1      2017-01-01 00:21:00 2017-01-01 00:57:00              3110   \n",
       "2      2017-01-01 00:22:00 2017-01-01 00:57:00              3110   \n",
       "3      2017-01-01 00:27:00 2017-01-01 00:39:00              3041   \n",
       "4      2017-01-01 00:28:00 2017-01-01 00:36:00              3047   \n",
       "...                    ...                 ...               ...   \n",
       "782335 2017-12-31 23:05:00 2017-12-31 23:33:00              3070   \n",
       "782336 2017-12-31 23:11:00 2018-01-01 11:03:00              3107   \n",
       "782337 2017-12-31 23:18:00 2017-12-31 23:25:00              3033   \n",
       "782338 2017-12-31 23:39:00 2017-12-31 23:40:00              3163   \n",
       "782339 2017-12-31 23:41:00 2017-12-31 23:55:00              3163   \n",
       "\n",
       "        end_station_id  bike_id user_type           start_station_name  \\\n",
       "0                 3041     5347  Indego30                 2nd & Market   \n",
       "1                 3054     3364   Walk-up  Del. River Trail & Penn St.   \n",
       "2                 3054     2536   Walk-up  Del. River Trail & Penn St.   \n",
       "3                 3005     5176  Indego30          Girard Station, MFL   \n",
       "4                 3124     5370   Walk-up       Independence Mall, NPS   \n",
       "...                ...      ...       ...                          ...   \n",
       "782335            3124     3708  Indego30   Spring Garden Station, MFL   \n",
       "782336            3165     5117  Indego30             33rd & Reservoir   \n",
       "782337            3046    11933  Indego30              10th & Chestnut   \n",
       "782338            3163     6725  Indego30                25th & Locust   \n",
       "782339            3057    11864  Indego30                25th & Locust   \n",
       "\n",
       "                  end_station_name  start_lat  start_lon    end_lat    end_lon  \n",
       "0              Girard Station, MFL  39.950119 -75.144722  39.968491 -75.135460  \n",
       "1                     Rodin Museum  39.961750 -75.136414  39.962502 -75.174202  \n",
       "2                     Rodin Museum  39.961750 -75.136414  39.962502 -75.174202  \n",
       "3                Welcome Park, NPS  39.968491 -75.135460  39.947330 -75.144028  \n",
       "4                 Race Street Pier  39.950710 -75.149208  39.952950 -75.139793  \n",
       "...                            ...        ...        ...        ...        ...  \n",
       "782335            Race Street Pier  39.960621 -75.139832  39.952950 -75.139793  \n",
       "782336             24th & Race SRT  39.982029 -75.188660  39.958191 -75.178200  \n",
       "782337                2nd & Market  39.950050 -75.156723  39.950119 -75.144722  \n",
       "782338               25th & Locust  39.949741 -75.180969  39.949741 -75.180969  \n",
       "782339  Philadelphia Museum of Art  39.949741 -75.180969  39.964581 -75.180031  \n",
       "\n",
       "[782340 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining df_philadelphia_2017 with df_philadelphia_full on start_time, end_time, bike_id and dropping duplicate columns\n",
    "df_philadelphia_2017_joined = df_philadelphia_2017.merge(df_philadelphia_website, on=[\"start_time\", \"end_time\", \"bike_id\"], how=\"left\")\n",
    "\n",
    "df_philadelphia_2017_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255784</th>\n",
       "      <td>2017-06-06 16:02:00</td>\n",
       "      <td>2017-06-06 16:33:00</td>\n",
       "      <td>3004</td>\n",
       "      <td>3161</td>\n",
       "      <td>5314</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>Municipal Services Building Plaza</td>\n",
       "      <td>30th Street Station East</td>\n",
       "      <td>39.953781</td>\n",
       "      <td>-75.163742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613438</th>\n",
       "      <td>2017-10-05 08:35:00</td>\n",
       "      <td>2017-10-05 08:47:00</td>\n",
       "      <td>3111</td>\n",
       "      <td>3107</td>\n",
       "      <td>5306</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>Parkside &amp; Belmont, Case Building</td>\n",
       "      <td>33rd &amp; Reservoir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.982029</td>\n",
       "      <td>-75.18866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613848</th>\n",
       "      <td>2017-10-05 10:31:00</td>\n",
       "      <td>2017-10-05 10:53:00</td>\n",
       "      <td>3095</td>\n",
       "      <td>3111</td>\n",
       "      <td>5329</td>\n",
       "      <td>Indego30</td>\n",
       "      <td>29th &amp; Diamond</td>\n",
       "      <td>Parkside &amp; Belmont, Case Building</td>\n",
       "      <td>39.987709</td>\n",
       "      <td>-75.180519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time            end_time  start_station_id  \\\n",
       "255784 2017-06-06 16:02:00 2017-06-06 16:33:00              3004   \n",
       "613438 2017-10-05 08:35:00 2017-10-05 08:47:00              3111   \n",
       "613848 2017-10-05 10:31:00 2017-10-05 10:53:00              3095   \n",
       "\n",
       "        end_station_id  bike_id user_type                 start_station_name  \\\n",
       "255784            3161     5314  Indego30  Municipal Services Building Plaza   \n",
       "613438            3107     5306  Indego30  Parkside & Belmont, Case Building   \n",
       "613848            3111     5329  Indego30                     29th & Diamond   \n",
       "\n",
       "                         end_station_name  start_lat  start_lon    end_lat  \\\n",
       "255784           30th Street Station East  39.953781 -75.163742        NaN   \n",
       "613438                   33rd & Reservoir        NaN        NaN  39.982029   \n",
       "613848  Parkside & Belmont, Case Building  39.987709 -75.180519        NaN   \n",
       "\n",
       "         end_lon  \n",
       "255784       NaN  \n",
       "613438 -75.18866  \n",
       "613848       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking for missing values in philadelphia data\n",
    "display(df_philadelphia_2017_joined[df_philadelphia_2017_joined.isnull().any(axis = 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_philadelphia_2017_joined.dropna(inplace = True)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding hour, month, weekday and trip duration(based on start and end time) to the table\n",
    "df_philadelphia_2017_joined.loc[:,\"trip_duration\"] = df_philadelphia_2017_joined[\"end_time\"] - df_philadelphia_2017_joined[\"start_time\"] \n",
    "df_philadelphia_2017_joined.loc[:,\"date\"] = df_philadelphia_2017_joined[\"start_time\"].dt.date\n",
    "df_philadelphia_2017_joined.loc[:,\"month\"] = df_philadelphia_2017_joined[\"start_time\"].dt.month\n",
    "df_philadelphia_2017_joined.loc[:,\"week\"] = df_philadelphia_2017_joined[\"start_time\"].dt.isocalendar().week\n",
    "df_philadelphia_2017_joined.loc[:,\"weekday\"] = df_philadelphia_2017_joined[\"start_time\"].dt.weekday\n",
    "df_philadelphia_2017_joined.loc[:,\"day\"] = df_philadelphia_2017_joined[\"start_time\"].dt.day\n",
    "df_philadelphia_2017_joined.loc[:,\"hour\"] = df_philadelphia_2017_joined[\"start_time\"].dt.hour\n",
    "df_philadelphia_2017_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove trips which are shorter than one minute and longer than one day because the bike sharing service does not allow those trips normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_philadelphia_2017_joined[df_philadelphia_2017_joined[\"trip_duration\"] < \"0 days 00:01:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_philadelphia_2017_joined = df_philadelphia_2017_joined[df_philadelphia_2017_joined[\"trip_duration\"] >= \"0 days 00:01:00\"]\n",
    "df_philadelphia_2017_joined[df_philadelphia_2017_joined[\"trip_duration\"] < \"0 days 00:01:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_philadelphia_2017_joined[df_philadelphia_2017_joined[\"trip_duration\"] > \"1 days\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_philadelphia_2017_joined = df_philadelphia_2017_joined[df_philadelphia_2017_joined[\"trip_duration\"] <= \"1 days\"]\n",
    "df_philadelphia_2017_joined[df_philadelphia_2017_joined[\"trip_duration\"] > \"1 days\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_philadelphia_cleaned = df_philadelphia_2017_joined.copy()\n",
    "df_philadelphia_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing and cleaning the weather dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing and cleaning the philadelphia dataset we need to do most of the procedure on the weather dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the type of date_time (df_weather) to datetime type from pandas\n",
    "df_weather.loc[:, \"date_time\"] = pd.to_datetime(df_weather[\"date_time\"])\n",
    "\n",
    "#Sort the dataframe by their corresponding time column\n",
    "df_weather.sort_values([\"date_time\"], inplace = True)\n",
    "\n",
    "#Reset the index\n",
    "df_weather.reset_index(drop = True, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we check for duplicate rows in the weather data and delete them if they occur because we do not need multiple rows multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for any duplicates in weather data\n",
    "print(\"Number of duplicates in df_weather: \", df_weather.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all duplicates for the weather data\n",
    "df_weather.drop_duplicates(subset= [\"date_time\"],inplace = True)\n",
    "\n",
    "display(df_weather[df_weather.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check for null values in the weather data and delete them. It is important to note that we later add additional rows with null values but with the missing time intervals because we can interpolate them with the interpolate function in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values in weather data\n",
    "display(df_weather[df_weather.isnull().any(axis = 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weather data consists of data from the beginning of 2015 until the end of 2019, we want to have as well only the weather data for 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting data from the beginning of 2017 till the end of 2017\n",
    "df_weather_2017 = df_weather[(df_weather[\"date_time\"]>= \"2017-01-01 00:00:00\") & (df_weather[\"date_time\"]< \"2018-01-01 00:00:00\" )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add missing intervals in the weather data\n",
    "df_weather_2017.set_index(\"date_time\", inplace = True)\n",
    "df_weather_2017 = df_weather_2017.resample(\"H\").asfreq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum temperature and maximum temperature will be interpolated linearly. The precipitation value is interpolated by taking one of the nearest existing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values in the weather data\n",
    "df_weather_2017[\"min_temp\"].interpolate(method = \"linear\", inplace = True)\n",
    "df_weather_2017[\"max_temp\"].interpolate(method = \"linear\", inplace = True)\n",
    "df_weather_2017[\"precip\"].interpolate(method = \"pad\", inplace = True)\n",
    "df_weather_2017.reset_index(drop=False, inplace = True)\n",
    "\n",
    "# Check if null values in the weather data still exist\n",
    "display(df_weather_2017[df_weather_2017.isnull().any(axis = 1)])\n",
    "display(df_weather_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dataframes for reference:\n",
    "* **Philadelphia data**\n",
    "   * *df_philadelphia_cleaned*\n",
    "* **Weather data**\n",
    "   * *df_weather_2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Demand Patterns and Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Rental during the day\n",
    "\n",
    "First count the started rentals for each hour of the day for the whole dataset and vizualize it in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of rentals in each hour (accumulated over the year)\n",
    "df = df_philadelphia_cleaned\n",
    "df.groupby([\"hour\"])[\"hour\"].count().reset_index(name=\"n_rentals_within_hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the mean\n",
    "mean = df.groupby([\"hour\"])[\"hour\"].count().mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"hour\"], bins=range(0,25,1), color=\"green\")\n",
    "plt.axhline(mean , color = 'r', linestyle = '--')\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Number of rentals started\")\n",
    "plt.title(\"Development of started rentals during the day\")\n",
    "plt.xticks(range(0,25,2))\n",
    "plt.grid(True)\n",
    "min_xlim, max_xlim = plt.xlim()\n",
    "plt.text(min_xlim*(-0.2), mean*1.1, 'Mean: {:.2f}'.format(mean))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get average values we calculate the number of days for which we have data in our dataset and calculate the avarage number of rentals started within each hour in a day.\n",
    "\n",
    "IMPORTANT: The average values only provide meaningful results if we assume that we have all meaningful rental transactions that actually occurred included in our data set or at least equally distributed missing transaction for each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of days for which we have data in our set\n",
    "number_days = len(df[\"date\"].unique())\n",
    "number_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average values for each hour\n",
    "df_avg_per_hour = df.groupby([\"hour\"])[\"hour\"].count().divide(number_days).reset_index(name=\"avg_n_rentals_within_hour\")\n",
    "df_avg_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean\n",
    "mean= df_avg_per_hour[\"avg_n_rentals_within_hour\"].mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "        df_avg_per_hour[\"hour\"], \n",
    "        df_avg_per_hour[\"avg_n_rentals_within_hour\"], \n",
    "        color=\"green\"\n",
    ")\n",
    "plt.axhline(mean , color = 'r', linestyle = '--')\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Average number of rentals started within hour\")\n",
    "plt.title(\"Development of avg started rentals during the day\")\n",
    "min_xlim, max_xlim = plt.xlim()\n",
    "plt.text(min_xlim*(-0.2), mean*1.1, 'Mean: {:.2f}'.format(mean))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plots above we only considered the starting times of the rentals. To also considere the endtime, we calculate how many bikes in the respective hours were IN USE accumulated over the year. (e.g. rental from 0 o'clock to 2:45 would count as a usage in hour 0, 1 and 2). In the process of calculation we also have to considere that bike rentals can range over two days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average number of bikes in use\n",
    "df[\"end_time_hour\"] = df[\"end_time\"].dt.hour\n",
    "df[\"end_time_date\"] = df[\"end_time\"].dt.date\n",
    "df_values = pd.DataFrame()\n",
    "for i in range(0,24,1):\n",
    "    df_values[f\"{i}\"]= (((df[\"hour\"] <= i) & (df[\"end_time_hour\"] >= i)) \n",
    "                        | ((df[\"end_time_hour\"] >= i) & (df[\"date\"] < df[\"end_time_date\"])))\n",
    "\n",
    "df_sum = df_values.apply(lambda x: x.sum()/number_days).reset_index(name=\"avg_n_of_bikes_in_use\").rename(columns={\"index\": \"hour\"})\n",
    "df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean\n",
    "mean = df_sum[\"avg_n_of_bikes_in_use\"].mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_sum[\"hour\"], df_sum[\"avg_n_of_bikes_in_use\"], color=\"green\")\n",
    "plt.axhline(mean , color = 'r', linestyle = '--')\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Average Number of Bikes in Use\")\n",
    "plt.title(\"Bikes in Use by Hours of the Day\")\n",
    "min_xlim, max_xlim = plt.xlim()\n",
    "plt.text(min_xlim*(-0.2), mean*1.1, 'Mean: {:.2f}'.format(mean))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis and Interpretation of the Results:\n",
    "\n",
    "The plots above all show that we have two peaks of bike rentals in the day. One peak demand at 8 am and the other peak at 5 pm. This peak can be a result of the rush-hour traffic. At these times most people are on their way to work/school or on their way back home. The demand around these times ( 8 am and 5 pm ) are also above average. Besides that, it also becomes clear that the bikes are mostly used during the day. During the night the demand is significantly below average (from 9 pm until 6 am)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Rental during the week\n",
    "\n",
    "First count the started rentals for each weekday of the day for the whole dataset and vizualize it in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accumulated rentals per weekday\n",
    "df.groupby([\"weekday\"])[\"weekday\"].count().reset_index(name=\"n_rentals_within_weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping: {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\",5: \"Saturday\",6: \"Sunday\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean\n",
    "mean = df.groupby([\"weekday\"])[\"weekday\"].count().mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"weekday\"], bins=range(0,8,1), color=\"green\")\n",
    "plt.axhline(mean , color = 'r', linestyle = '--')\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Total Number of Rentals Started on Weekday\")\n",
    "plt.title(\"Development of Started Rentals During the Week\")\n",
    "plt.xticks(range(0,8,1))\n",
    "min_xlim, max_xlim = plt.xlim()\n",
    "plt.text(min_xlim*(-0.1), mean*1.1, 'Mean: {:.2f}'.format(mean))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get average values we calculate the number of weeks for which we have data in our dataset and calculate the avarage number of rentals started within each weekday.\n",
    "\n",
    "IMPORTANT ASSUMPTION: The average values only provide meaningful results if we assume that we have all rental transactions that actually occurred included in our data set or at least equally distributed missing data points for each weekday. Otherwise the average values would provide misleading information. If we for example would have missing rental data for a specific weekday for several weeks, dividing by the total number of weeks would result to false average value for this weekday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of weeks we have in our set\n",
    "number_weeks = len(df[\"week\"].unique())\n",
    "number_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average values per weekday\n",
    "df_average_per_weekday = df.groupby([\"weekday\"])[\"weekday\"].count().divide(number_weeks).reset_index(name=\"avg_number_started_rentals\")\n",
    "df_average_per_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean\n",
    "mean = df_average_per_weekday[\"avg_number_started_rentals\"].mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "        df_average_per_weekday[\"weekday\"], \n",
    "        df_average_per_weekday[\"avg_number_started_rentals\"], \n",
    "        color=\"green\"\n",
    ")\n",
    "plt.axhline(mean, color=\"r\", linestyle=\"--\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Average Number of Rentals Started per Weekday\")\n",
    "plt.title(\"Development of Started Rentals During the Week\")\n",
    "min_xlim, max_xlim = plt.xlim()\n",
    "plt.text(min_xlim*(-0.1), mean*1.1, 'Mean: {:.2f}'.format(mean))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis and Interpretation of Results:\n",
    "\n",
    "The Bike rental demand is the lowest at the weekends (Saturdays and Sundays). The demand these days is significantly below average. The demand on Mondays is slightly below the average. We have a peak demand on Wednesdays. This development can possibly be rooted in the fact that most people don't work/ go to school or university on the weekends. This could mean that a big share of people who use the bike rental service are people who are on their way to work, school, or university."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Rental during the Year\n",
    "\n",
    "First count the started rentals for each month of the year for the whole dataset and vizualize it in a histogram.\n",
    "Average values can not be calculated since we only have data of one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate number of rentals per month\n",
    "df.groupby([\"month\"])[\"month\"].count().reset_index(name=\"n_of_retals_in_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean\n",
    "mean = df.groupby([\"month\"])[\"month\"].count().mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"month\"], bins=range(0,14,1), color=\"green\")\n",
    "plt.axhline(mean, color=\"r\", linestyle=\"--\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Total Number of Rentals Started in Month\")\n",
    "plt.title(\"Development of Started Rentals during Month\")\n",
    "plt.xticks(range(1,13,1))\n",
    "min_xlim, max_xlim = plt.xlim()\n",
    "plt.text(min_xlim*(-0.1), mean*(1.06), \"Mean: {:.2f}\".format(mean))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather_2017\n",
    "df_weather[\"date\"] = df_weather[\"date_time\"].dt.date\n",
    "df_weather[\"hour\"] = df_weather[\"date_time\"].dt.hour\n",
    "\n",
    "#Join weather data with rental data and calculate average temperatures\n",
    "df_merge = pd.merge(df, df_weather, how=\"left\", left_on=[\"date\", \"hour\"], right_on=[\"date\", \"hour\"])\n",
    "df_merge[\"avg_temp\"] = df_merge[[\"max_temp\", \"min_temp\"]].mean(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average temperatures for each month\n",
    "df_temp_month = df_merge.groupby(\"month\")[\"avg_temp\"].mean().reset_index(name=\"avg_temp\")\n",
    "df_temp_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot number of rentals together with average temperature\n",
    "#Red dots would be the average temperature for each month\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "df_months = df.groupby([\"month\"])[\"month\"].count().reset_index(name=\"n_of_retals_in_month\")\n",
    "ax1.bar(df_months[\"month\"], df_months[\"n_of_retals_in_month\"], color=\"green\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "ax1.set_ylabel(\"Total Number of Rentals Started in Month\")\n",
    "plt.title(\"Development of Started Rentals and Temperature\")\n",
    "plt.xticks(range(1,13,1))\n",
    "ax2.scatter(df_temp_month[\"month\"], df_temp_month[\"avg_temp\"], color=\"red\")\n",
    "ax2.set_ylabel(\"Average Temperature\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_temp_month[\"avg_temp\"], df_months[\"n_of_retals_in_month\"], color=\"red\")\n",
    "plt.xlabel(\"Average Temperature\")\n",
    "plt.ylabel(\"Bike Rental Demand\")\n",
    "plt.title(\"Development of Bike Rental Demand dependant on Average Temperature\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis and Interpretation of the Results:\n",
    "\n",
    "The development of bike rental demand has similar development as the development of the average temperature during the year. \n",
    "The only difference can be seen at the peak. The average temperature has its peak in July whereas the bike rental demand has its peak in August. However, the difference in the bike rental demand in July and August is not that high. Another effect that could have an influence on the bike rental demand is the number of tourists. According to the annual report of Philadelphia, the number of tourists was the highest for the second and third quarters of the year (April to September). In these months the bike rental demand was always very close (April) or above the overall average of the bike rental demand. \n",
    "Especially for the month of December, January, February, and March the bike rental demand is clearly below the average.\n",
    "\n",
    "Development of the tourist numbers according to the annual report: \n",
    "- Q1: 7 million\n",
    "- Q2: 13.1 million\n",
    "- Q3: 13.1 million\n",
    "- Q4: 10.1 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Performance Indicators (KPIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rental Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rental durations are closely tied to revenue because the revenue model of the bike sharing company is based on the rental time/ duration thus its relevant to observe the rental duration to get information on the company's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate the sum of rental durations for each month\n",
    "#Calculate the average rental duration per rental for each month\n",
    "df = df_philadelphia_cleaned\n",
    "df_duration_monthly = df.groupby(\"month\")[\"trip_duration\"].sum().reset_index(name=\"sum_duration_bike_rentals\")\n",
    "df_duration_monthly[\"duration_in_hours\"] = df_duration_monthly[\"sum_duration_bike_rentals\"].dt.total_seconds().divide(60*60)\n",
    "df_duration_monthly[\"duration_in_minutes\"] = df_duration_monthly[\"sum_duration_bike_rentals\"].dt.total_seconds().divide(60)\n",
    "df_duration_monthly[\"number_of_rentals\"] = df.groupby(\"month\")[\"month\"].count().reset_index(drop=True)\n",
    "df_duration_monthly[\"avg_duration_per_rental_in_min\"] = df_duration_monthly[\"duration_in_minutes\"] / df_duration_monthly[\"number_of_rentals\"]\n",
    "df_duration_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize development of summed durations\n",
    "plt.bar(df_duration_monthly[\"month\"],\n",
    "       df_duration_monthly[\"duration_in_hours\"],\n",
    "       color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1,13,1))\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Sum of Rental Durations\")\n",
    "plt.title(\"Development of the Summed Rental Durations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize development of average durations for each month\n",
    "plt.bar(df_duration_monthly[\"month\"],\n",
    "       df_duration_monthly[\"avg_duration_per_rental_in_min\"],\n",
    "       color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1,13,1))\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Avg Duration per Rental in min\")\n",
    "plt.title(\"Development of Average Rental Duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the sum of rental durations for each week\n",
    "#Calculate the average rental duration per rental for each week\n",
    "df_duration_weekly = df.groupby(\"week\")[\"trip_duration\"].sum().reset_index(name=\"sum_duration_bike_rentals\")\n",
    "df_duration_weekly[\"duration_in_hours\"] = df_duration_weekly[\"sum_duration_bike_rentals\"].dt.total_seconds().divide(60*60)\n",
    "df_duration_weekly[\"duration_in_minutes\"] = df_duration_weekly[\"sum_duration_bike_rentals\"].dt.total_seconds().divide(60)\n",
    "df_duration_weekly[\"number_of_rentals\"] = df.groupby(\"week\")[\"week\"].count().reset_index(drop=True)\n",
    "df_duration_weekly[\"avg_duration_per_rental_in_min\"] = df_duration_weekly[\"duration_in_minutes\"] / df_duration_weekly[\"number_of_rentals\"]\n",
    "df_duration_weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize development of summed durations\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "plt.plot(df_duration_weekly[\"week\"],\n",
    "       df_duration_weekly[\"duration_in_hours\"],\n",
    "       color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1,53,1))\n",
    "plt.xlabel(\"Weeks\")\n",
    "plt.ylabel(\"Sum of Rental Durations\")\n",
    "plt.title(\"Development of the Summed Rental Durations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Vizualize development of average durations for each week\n",
    "plt.plot(df_duration_weekly[\"week\"],\n",
    "       df_duration_weekly[\"avg_duration_per_rental_in_min\"],\n",
    "       color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1,53,1))\n",
    "plt.xlabel(\"Weeks\")\n",
    "plt.ylabel(\"Average of Rental Durations\")\n",
    "plt.title(\"Development of the Average Rental Durations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the Results\n",
    "\n",
    "- The summed rental durations have a similar development as the summed number of rentals \n",
    "- Our bikes are more used in the summer months especially from June to October\n",
    "- This can be seen in the monthly as well as the weekly aggregation \n",
    "- The average rental duration per ride lies between 15 and 22 minutes with the average rental duration being especially high from April to July"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rental Duration Dependant on Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather_2017\n",
    "df_weather[\"date\"] = df_weather[\"date_time\"].dt.date\n",
    "df_weather[\"hour\"] = df_weather[\"date_time\"].dt.hour\n",
    "\n",
    "#Join weather data with rental data and calculate average temperatures\n",
    "df_merge = pd.merge(df, df_weather, how=\"left\", left_on=[\"date\", \"hour\"], right_on=[\"date\", \"hour\"])\n",
    "df_merge[\"avg_temp\"] = df_merge[[\"max_temp\", \"min_temp\"]].mean(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_duration_daily = df_merge.groupby(\"date\").agg({\"trip_duration\": lambda x: x.sum(), \n",
    "                                                  \"precip\": lambda x: x.sum(), \n",
    "                                                  \"avg_temp\": lambda x: x.mean()})\n",
    "df_duration_daily[\"duration_in_hours\"] = df_duration_daily[\"trip_duration\"].dt.total_seconds().divide(60*60)\n",
    "df_duration_daily[\"duration_in_minutes\"] = df_duration_daily[\"trip_duration\"].dt.total_seconds().divide(60)\n",
    "df_duration_daily[\"number_of_rentals\"] = df.groupby(\"date\")[\"date\"].count()\n",
    "df_duration_daily[\"avg_duration_per_rental_in_min\"] = df_duration_daily[\"duration_in_minutes\"] / df_duration_daily[\"number_of_rentals\"]\n",
    "df_duration_daily_precip = df_duration_daily[df_duration_daily[\"precip\"] > 0]\n",
    "df_duration_daily_no_precip = df_duration_daily[df_duration_daily[\"precip\"] == 0]\n",
    "df_duration_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average summed rental duration per day\n",
    "avg_summed_duration_precip = df_duration_daily_precip[\"duration_in_hours\"].mean()\n",
    "avg_summed_duration_no_precip = df_duration_daily_no_precip[\"duration_in_hours\"].mean()\n",
    "print(f\"{avg_summed_duration_precip= }\\n{avg_summed_duration_no_precip= }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate average rental duration per rental\n",
    "avg_duration_per_rental_precip = df_duration_daily_precip[\"avg_duration_per_rental_in_min\"].mean()\n",
    "avg_duration_per_rental_no_precip = df_duration_daily_no_precip[\"avg_duration_per_rental_in_min\"].mean()\n",
    "print(f\"{avg_duration_per_rental_precip= }\\n{avg_duration_per_rental_no_precip= }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average rental duration and average summed rental duration per day dependen on temperature\n",
    "df_duration_daily[\"temp_bin\"] = pd.cut(df_duration_daily.avg_temp, 10)\n",
    "df_temp_bins = df_duration_daily.groupby(\"temp_bin\").agg({\"duration_in_hours\": lambda x: x.mean(), \n",
    "                                          \"avg_duration_per_rental_in_min\": lambda x: x.mean()}).reset_index()\n",
    "df_temp_bins[\"temp_bin\"] = df_temp_bins[\"temp_bin\"].astype(str)\n",
    "df_temp_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "plt.bar(df_temp_bins[\"temp_bin\"], df_temp_bins[\"duration_in_hours\"], color=\"green\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Average Summed Trip Durations per Day in Hours\")\n",
    "plt.title(\"Development of Average Summed Trip Durations per Day Dependent on Temperature\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_temp_bins[\"temp_bin\"], df_temp_bins[\"avg_duration_per_rental_in_min\"], color=\"green\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Average Trip Duration in Minutes\")\n",
    "plt.title(\"Development of Average Trip Durations Dependent on Temperature\")\n",
    "plt.grid(True)\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the Results\n",
    "\n",
    "- When additionally taking weather data into account, we can observe that the average summed rental durations per day are lower on days with precipitation. This is also the case for the average rental duration per day\n",
    "- Besides that, the average summed rental durations per day also increase with the increase of the temperature\n",
    "- With the average rental duration per trip we only slightly see this effect (there is not a continuous increase and not a very extreme one as with summed rental durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bike Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bike utilization gives us information on how much the different bikes are used.\n",
    "This information is relevant to get information about possible unutilized /unused bikes which are maybe actually not needed (unused ressources/capacitiy).\n",
    "Moreover we analyse how many unique bikes at which time and at which weather condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#approximate how many individual bikes there are (we have no information about total number of bikes)\n",
    "number_of_bikes = len(df[\"bike_id\"].unique())\n",
    "total_number_of_rentals = len(df) # each row is a rental\n",
    "print(f\"{number_of_bikes=}, {total_number_of_rentals=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate total number of rides for each bike over the whole year\n",
    "df_bike_ride_count = df.groupby(\"bike_id\")[\"start_time\"].count().reset_index(name=\"total_number_of_rides\").sort_values(by=\"total_number_of_rides\", ascending=False).reset_index(drop=True)\n",
    "df_bike_ride_count[\"cumulative_ride_count\"] = df_bike_ride_count[\"total_number_of_rides\"].cumsum()\n",
    "df_bike_ride_count[\"percentage_number_of_bikes\"] = df_bike_ride_count.index.to_series().add(1).divide(number_of_bikes)\n",
    "df_bike_ride_count[\"percentage_ride_count\"] = df_bike_ride_count[\"cumulative_ride_count\"].divide(total_number_of_rentals)\n",
    "df_bike_ride_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.plot(df_bike_ride_count.index.to_series(), df_bike_ride_count[\"total_number_of_rides\"], color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Total Number of Rides\")\n",
    "plt.xlabel(\"Bike Count\")\n",
    "plt.title(\"Total Number of Rides per Bike\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_bike_ride_count.index.to_series(), df_bike_ride_count[\"cumulative_ride_count\"], color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Bike Count\")\n",
    "plt.ylabel(\"Number of Rentals\")\n",
    "plt.title(\"Cumulative Number of Rentals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_bike_ride_count[\"percentage_number_of_bikes\"], df_bike_ride_count[\"percentage_ride_count\"], \n",
    "         color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Number of Bikes in Percent\")\n",
    "plt.ylabel(\"Number of Rentals in Percent\")\n",
    "plt.title(\"Number of Bikes and Rentals in Percent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilization by month\n",
    "df_util_month = df.groupby(\"month\")[\"bike_id\"].nunique().reset_index(name=\"number_of_bikes_used\")\n",
    "df_util_month[\"utilization\"] = df_util_month[\"number_of_bikes_used\"].divide(number_of_bikes)\n",
    "df_util_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_util_month[\"month\"], df_util_month[\"number_of_bikes_used\"], color=\"green\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Number of Used Bikes\")\n",
    "plt.title(\"Utilization of Bikes per Month\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_util_month[\"month\"], df_util_month[\"utilization\"], color=\"green\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Utilization in Percent\")\n",
    "plt.title(\"Utilization of Bikes per Month\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our initial assumption of the total number of bikes could be untrue for the different months.\n",
    "It could be possible that bikes are added in some months. For example in Mai there is an sudden increase of the bike utilization. \n",
    "We will check it in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[df[\"month\"]==5][\"bike_id\"].unique()) - set(df[df[\"month\"]==4][\"bike_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It seems that bikes above 11700 are added in Mai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for bikes with BikeID above 11700 for months 1 to 4\n",
    "for i in range(1,5):\n",
    "    print((df[df[\"month\"]==i][\"bike_id\"] > 11700).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the highest BikeID for each month\n",
    "df.groupby(\"month\")[\"bike_id\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekly values\n",
    "df_bike_count_weekly = df.groupby(\"week\")[\"bike_id\"].nunique().reset_index(name=\"number_of_bikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_bike_count_weekly[\"week\"], df_bike_count_weekly[\"number_of_bikes\"], color=\"green\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Number of Used Bikes\")\n",
    "plt.title(\"Number of Used Bikes per Week\")\n",
    "plt.xticks(range(0,53,2))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daily values\n",
    "df_bike_count_daily = df.groupby([\"date\"])[\"bike_id\"].nunique().reset_index(name=\"number_of_bikes\")\n",
    "df_bike_count_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.plot(df_bike_count_daily[\"date\"], df_bike_count_daily[\"number_of_bikes\"], color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.yticks(range(0,1000,100))\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Number of Bikes\")\n",
    "plt.title(\"Number of Bikes Used per Day\")\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average bike utilization by weekday\n",
    "df_bike_count_daily[\"date\"] = pd.to_datetime(df_bike_count_daily[\"date\"])\n",
    "df_bike_count_daily[\"weekday\"] = df_bike_count_daily[\"date\"].dt.weekday\n",
    "df_weekday = df_bike_count_daily.groupby(\"weekday\")[\"number_of_bikes\"].mean().reset_index(name=\"avg_n_bikes\")\n",
    "df_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_weekday[\"weekday\"], df_weekday[\"avg_n_bikes\"], color=\"green\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Number of Bikes\")\n",
    "plt.title(\"Average Number of Bikes per Weekday\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average bike utilization by hour of day\n",
    "df_bike_count_hourly = df.groupby([\"date\", \"hour\"])[\"bike_id\"].nunique().reset_index(name=\"number_of_bikes\")\n",
    "df_hour_of_day = df_bike_count_hourly.groupby(\"hour\")[\"number_of_bikes\"].mean().reset_index(name=\"avg_n_bikes\")\n",
    "df_hour_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_hour_of_day[\"hour\"], df_hour_of_day[\"avg_n_bikes\"], color=\"green\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Number of Bikes\")\n",
    "plt.title(\"Average Number of Used Bikes per Hour\")\n",
    "plt.xticks(range(0,24,1))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the Results\n",
    "\n",
    "- We approximated which number of total bikes there could be in the fleet \n",
    "- We  found a total of 1249 individual bikes in the data set \n",
    "- However, there are likely bikes added over time, since some bikeID's (especially higher ones) only occur starting at a certain month in the year\n",
    "- This fact has to be considered when analyzing the results\n",
    "\n",
    "- When analyzing the utilization of the individual bikes we can observe that only a few bikes (about five) are used significantly more than the others. And about two hundred bikes are used less than most bikes. But for most bikes, as seen in the graph the utilization does not vary too much. Besides that, because some bikes are likely to be added over time, it makes sense that these bikes were not used as much as the rest of the bikes.\n",
    "\n",
    "- The bike utilization per month (considering a total number of bikes of 1249) shows that it was likely that bikes were added for example in Mai. Since we can see a sudden increase in the utilization (in the months before the utilization was quite the same)\n",
    "\n",
    "- When analyzing the average number of bikes used per weekday it becomes clear that there are on average more bikes used during the week than during the weekends. Similar to the development of the number of rides.\n",
    "- The development during the day also shows similar results to the development of the number of rides. We have clear peaks at 8 am and 5 pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bike Utilization Dependent on Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average number of bikes used on hour with precipitation vs. one with none\n",
    "df_hourly = df_merge.groupby([\"date\", \"hour\"]).agg({\"bike_id\": lambda x: x.nunique(), \n",
    "                                                    \"precip\": lambda x: x.sum(),\n",
    "                                                   \"avg_temp\": lambda x: x.mean()})\n",
    "df_hourly = df_hourly.rename(columns={\"bike_id\": \"n_bikes_used\"})\n",
    "df_no_precip_hourly = df_hourly[df_hourly[\"precip\"] == 0]\n",
    "df_precip_hourly = df_hourly[df_hourly[\"precip\"] > 0]\n",
    "print(f\"with precip: {df_precip_hourly['n_bikes_used'].mean()} avg bikes used per hour\\nwithout: {df_no_precip_hourly['n_bikes_used'].mean()} avg bikes used per hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average number of bikes used on hour dependent on temperature\n",
    "df_hourly[\"temp_bin\"] = pd.cut(df_hourly[\"avg_temp\"], 10)\n",
    "df_temp_bins_hourly = df_hourly.groupby(\"temp_bin\")[\"n_bikes_used\"].mean().reset_index(name=\"avg_n_bikes_per_hour\")\n",
    "df_temp_bins_hourly[\"temp_bin\"] = df_temp_bins_hourly[\"temp_bin\"].astype(str)\n",
    "df_temp_bins_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "plt.bar(df_temp_bins_hourly[\"temp_bin\"], df_temp_bins_hourly[\"avg_n_bikes_per_hour\"], color=\"green\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Average Number of Bikes Used per Hour\")\n",
    "plt.title(\"Average Number of Bikes Used per Hour Dependant on Temperature\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average number of bikes used on day with precipitation vs. one with none\n",
    "df_daily = df_merge.groupby(\"date\").agg({\"bike_id\": lambda x: x.nunique(), \n",
    "                                         \"precip\": lambda x: x.sum(),\n",
    "                                         \"avg_temp\": lambda x: x.mean()})\n",
    "df_daily = df_daily.rename(columns={\"bike_id\": \"n_bikes_used\"})\n",
    "df_no_precip_daily = df_daily[df_daily[\"precip\"] == 0]\n",
    "df_precip_daily = df_daily[df_daily[\"precip\"] > 0]\n",
    "print(f\"with precip: {df_precip_daily['n_bikes_used'].mean()} avg bikes used per day\\nwithout: {df_no_precip_daily['n_bikes_used'].mean()} avg bikes used per day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average number of bikes used on day dependent on temperature\n",
    "df_daily[\"temp_bin\"] = pd.cut(df_daily[\"avg_temp\"], 10)\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_bins = df_daily.groupby(\"temp_bin\")[\"n_bikes_used\"].mean().reset_index(name=\"avg_n_bikes_per_day\")\n",
    "df_temp_bins[\"temp_bin\"] = df_temp_bins[\"temp_bin\"].astype(str)\n",
    "df_temp_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_temp_bins[\"temp_bin\"], df_temp_bins[\"avg_n_bikes_per_day\"], color=\"green\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Average Number of Bikes Used per Day\")\n",
    "plt.title(\"Average Number of Bikes Used per Day Dependant on Temperature\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the Results\n",
    "\n",
    "The average number of bikes used per hour is lower when precipitationis recorded. Besides that, we see a steady increase in the average number of bikes used per hour with the increase in the temperature. This is also the case for the average number of bikes used per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features\n",
    "- day_of_week \n",
    "- hour_of_day \n",
    "- month \n",
    "- avg_temp (min and max temp don't differ much)\n",
    "- precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose day of the week as a feature because it became clear in the descriptive analytics part that the weekday has an effect on the bike-rental demand. Especially on the weekends the demand is much lower. It would also be possible to convert the variable into a binary one (weekend) to only check if the respective day falls into the weekend. However, we would then lose some information because even on some days during the week the demand is slightly higher than on other days. Besides that Du et. al. also used the feature day of the week to forecast bike rental demand.  \n",
    "\n",
    "The descriptive analytics part also showed that the hour of the day has also an effect on the rental demand. Especially during the night the demand is low and we have peaks at the rush hours (8 am and 5 pm).  \n",
    "\n",
    "Besides that, the demand seems to differ dependent on the month. For the summer months, we clearly see an increase in the bike rental demand.   To use this information we decided to include it as a feature in our models.\n",
    "The min and max temperature for one hour do not differ much and thus have a high correlation which we wanted to remove. Thus we decided to work with the average temperature for the respective hours.  \n",
    "\n",
    "Besides that precipitation seems to also have an effect on the demand. The number of rentals and also the summed rental durations are higher for no precipitation records.\n",
    "\n",
    "Du et al. paper from Standford Univesity: http://cs229.stanford.edu/proj2014/Jimmy%20Du,%20Rolland%20He,%20Zhivko%20Zhechev,%20Forecasting%20Bike%20Rental%20Demand.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df_philadelphia_cleaned.copy()\n",
    "df = df.set_index(\"start_time\")\n",
    "df = df.loc[:, [\"day\", \"month\", \"weekday\", \"hour\", \"end_time\"]]\n",
    "df = df.resample(\"H\").agg({\n",
    "    \"day\" : \"max\",\n",
    "    \"month\": \"max\",\n",
    "    \"weekday\": \"max\",\n",
    "    \"hour\": \"max\",\n",
    "    \"end_time\": \"count\"\n",
    "})\n",
    "df = df.rename(columns={\"end_time\": \"demand\"})\n",
    "df.index.names = [\"time\"]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.apply(lambda x: x.isnull().any(), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the weather data\n",
    "df_weather = df_weather_2017.copy()\n",
    "df_weather = df_weather.loc[:, [\"date_time\",\"max_temp\", \"min_temp\", \"precip\"]]\n",
    "df_merge = pd.merge(df, df_weather, left_index=True, right_on=\"date_time\").set_index(\"date_time\")\n",
    "df_merge[\"avg_temp\"] = df_merge[[\"max_temp\", \"min_temp\"]].mean(axis=1)\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into dependent and independent variables\n",
    "y = df_merge[\"demand\"]\n",
    "X = df_merge.drop([\"demand\", \"min_temp\", \"max_temp\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test and train and validation set\n",
    "\n",
    "# Do a 70-30 split first\n",
    "X_train_hold, X_test, y_train_hold, y_test = train_test_split(X, y, test_size=0.3,random_state=34)\n",
    "\n",
    "# now split X_train to achive 50-20-30 split\n",
    "X_train, X_hold, y_train, y_hold = train_test_split(X_train_hold, y_train_hold, test_size=(0.2/0.7),random_state=34)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function\n",
    "\n",
    "def plot_prediction(X_tr, y_tr, X_te, y_pr, column):\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(X_tr[column], y_tr, marker=\"x\", c='C1')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"demand\")\n",
    "    plt.scatter(X_te[column], y_pr, marker=\"x\", c='red')\n",
    "    plt.legend(['Observed data', 'fitted data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics function\n",
    "\n",
    "def get_metrics(y_train, y_pred_train, y_test, y_pred_test):\n",
    "\n",
    "    return {\n",
    "            \"MAE\": {\n",
    "                    \"train\": mean_absolute_error(y_train, y_pred_train),\n",
    "                    \"test\": mean_absolute_error(y_test, y_pred_test)\n",
    "                }, \n",
    "            \"r2\": {\n",
    "                    \"train\": r2_score(y_train, y_pred_train),\n",
    "                    \"test\": r2_score(y_test, y_pred_test)\n",
    "                }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficients\n",
    "\n",
    "def get_coefficients(model, columns):\n",
    "        return pd.DataFrame(\n",
    "                    {\n",
    "                        \"variable\" : columns,\n",
    "                        \"coef\": model.coef_\n",
    "                    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression function\n",
    "\n",
    "def linear_regression(X_train, y_train, X_test, y_test, plot, column, get_coef=None):   \n",
    "    #Fitting\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    \n",
    "    #Prediction on testing and training set\n",
    "    y_pred_test = lin_reg.predict(X_test)\n",
    "    y_pred_train = lin_reg.predict(X_train)\n",
    "    y_pred_test[y_pred_test < 0] = 0\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "    \n",
    "    if(plot):\n",
    "        plot_prediction(X_train, y_train, X_train, y_pred_train, column)\n",
    "        \n",
    "    return (get_metrics(y_train, y_pred_train, y_test, y_pred_test),\n",
    "            get_coefficients(lin_reg, X_train.columns) if get_coef else None,\n",
    "            y_pred_test,\n",
    "            y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics, coef, _, __ = linear_regression(X_train_hold, y_train_hold, X_test, y_test, True, \"avg_temp\", True)\n",
    "display(metrics, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Linear Regression with dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of linear regression would not yield good results with the feature set we have. The descriptive analytics\tpart with its plots clearly shows that most features do not have a linear relationship with the dependent variable (bike rental demand). For example, for the feature of hour_of_day, we have two peaks of the bike rental demand which can not be represented bike a linear function. The only feature that seems to have a linear relationship with the dependent variable is the average temperature. We see an increasing demand with increasing temperature. \n",
    "But if we see the features day_of_week, hour_of_day, and month as categorical features, we could actually yield much better results. We decided to convert each of these features into multiple dummy variables. By doing so we would generate in the end linear model with multiple intercepts.\n",
    "\n",
    "This approach has the following drawbacks: \n",
    "- we would increase the model complexity by increasing dimensionality\n",
    "- we would lose the ordering of the features (features are actually ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_hold.loc[:,[\"day\", \"month\",\"weekday\", \"hour\"]] = X_train_hold.loc[:,[\"day\", \"month\", \"weekday\", \"hour\"]].astype(str)\n",
    "X_test.loc[:,[\"day\", \"month\", \"weekday\", \"hour\"]] = X_test.loc[:,[\"day\", \"month\", \"weekday\", \"hour\"]].astype(str)\n",
    "X_train_dum = pd.get_dummies(X_train_hold)\n",
    "X_test_dum = pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, coef, _, __ = linear_regression(X_train_dum, y_train_hold, X_test_dum, y_test, True, \"avg_temp\", True)\n",
    "display(metrics, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Results:\n",
    "- the r2 score gives as information about the proportion of the variance of the dependent variable (the bike-rental demand) which can be explained by our model (the independent variables). This in this case for the test set 72.46 % of the variance of the bike rental demand can be explained through our model.\n",
    "- besides that the means absolute error (how far was our prediction of the bike rental demand on average away from the actual value) is 31.46\n",
    "- the coefficents for the different dummy variables can be interpreted as the different intercepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above we have some features for which we donâ€™t\thave a linear relationship with the dependent variable. This non-linear relationship can be modeled using polynomial features through which we can better approximate the non-linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_regression(X_train, y_train, X_test, y_test, degree, plot=None, column=None):   # method can be r2 or MAE\n",
    "    \n",
    "    # initialize model\n",
    "    poly_reg = PolynomialFeatures(degree)\n",
    "\n",
    "    # fit and transform\n",
    "    X_poly = poly_reg.fit_transform(X_train)\n",
    "    X_poly_test = poly_reg.fit_transform(X_test)\n",
    "    \n",
    "    return linear_regression(X_poly, y_train, X_poly_test, y_test, plot, column)\n",
    "    \n",
    "\n",
    "\n",
    "def find_poly_regression(X_train, y_train, X_hold, y_hold, max_value):\n",
    "    \n",
    "    mae = {\"hold\": [], \"train\": []}\n",
    "    r2 = {\"hold\": [], \"train\": []}\n",
    "    \n",
    "    array = np.arange(1,max_value +1)\n",
    "    \n",
    "    for i in array:\n",
    "        metrics, _, __,___ = poly_regression(X_train, y_train, X_hold, y_hold, i)\n",
    "        mae[\"hold\"].append(metrics[\"MAE\"][\"test\"]) \n",
    "        mae[\"train\"].append(metrics[\"MAE\"][\"train\"])\n",
    "        r2[\"hold\"].append(metrics[\"r2\"][\"test\"]) \n",
    "        r2[\"train\"].append(metrics[\"r2\"][\"train\"])\n",
    "    \n",
    "    # plot results\n",
    "    \n",
    "    # plot results\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.plot(array, mae[\"train\"], label=\"train\")\n",
    "    plt.plot(array, mae[\"hold\"], label=\"hold\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of polynomials\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.plot(array, r2[\"train\"], label=\"train\")\n",
    "    plt.plot(array, r2[\"hold\"], label=\"hold\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of polynomials\")\n",
    "    plt.ylabel(\"r2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_poly_regression(X_train, y_train, X_hold, y_hold, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_poly_regression(X_train, y_train, X_hold, y_hold, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selection of the degree of polynomials\n",
    "For the polynomial of degree 5 we seem to yield the best results regarding the r2 score and mean absolute error. For a polynomial of degree 6 we can see that the mean absolut error rises and the r2 score drops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, _, y_pred_test, y_pred_train = poly_regression(X_train, y_train, X_test, y_test, 5)\n",
    "display(metrics)\n",
    "plot_prediction(X_train, y_train, X_train, y_pred_train, \"avg_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "- the r2 score gives as information about the proportion of the variance of the dependent variable (the bike-rental demand) which can be explained by our model (the independent variables). This in this case for the test set 64.40 % of the variance of the bike rental demand can be explained through our model.\n",
    "- besides that the means absolute error (how far was our prediction of the bike rental demand on average away from the actual value) is 35.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(X_train, y_train, X_train, y_pred_train, \"month\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "62678a3150b6c42bd748e0bf3f0d349f6ce37b8345d36615de7e3b4f366186fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
